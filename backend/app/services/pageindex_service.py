"""
PageIndex service for document indexing and search
"""
import os
import json
import sys
import logging
from pathlib import Path
from typing import Dict, Optional, Any
from app.core.config import settings

logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# –ö–†–ò–¢–ò–ß–ù–û: –ü–∞—Ç—á–∏–º PageIndex –¥–ª—è Ollama –ü–ï–†–ï–î –∏–º–ø–æ—Ä—Ç–æ–º
try:
    from pageindex_ollama import patch_pageindex_for_ollama, check_ollama_connection
    
    logger.info(f"üîß –ù–∞—á–∏–Ω–∞—é –ø–∞—Ç—á–∏–Ω–≥ PageIndex –¥–ª—è Ollama (–º–æ–¥–µ–ª—å: {settings.OLLAMA_MODEL})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–∞—Ç—á–∏–º
    if not patch_pageindex_for_ollama(
        base_url=settings.OLLAMA_BASE_URL,
        model=settings.OLLAMA_MODEL
    ):
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å PageIndex –¥–ª—è Ollama")
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å PageIndex –¥–ª—è Ollama")
    else:
        logger.info(f"‚úÖ PageIndex —É—Å–ø–µ—à–Ω–æ –ø–∞—Ç—á–µ–Ω –¥–ª—è Ollama (–º–æ–¥–µ–ª—å: {settings.OLLAMA_MODEL})")
except ImportError as e:
    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å pageindex_ollama: {e}")
    raise
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ç—á–∏–Ω–≥–µ PageIndex: {e}")
    import traceback
    logger.error(traceback.format_exc())
    raise

# –¢–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º PageIndex (—É–∂–µ —Å –ø–∞—Ç—á–µ–º)
try:
    # –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ PageIndex
    from PageIndex.pageindex import page_index_main, config
    # –ö–†–ò–¢–ò–ß–ù–û: –ü–∞—Ç—á–∏–º —Ç–∞–∫–∂–µ page_index –º–æ–¥—É–ª—å, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "from .utils import *"
    try:
        import PageIndex.pageindex.page_index as page_index_module
        from pageindex_ollama import get_ollama_settings
        ollama_settings = get_ollama_settings()
        if ollama_settings.get('patched'):
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á–∏–Ω–≥ –∫ page_index –º–æ–¥—É–ª—é
            import openai
            ollama_client = openai.OpenAI(
                api_key="ollama",
                base_url=ollama_settings['base_url']
            )
            ollama_async_client = openai.AsyncOpenAI(
                api_key="ollama",
                base_url=ollama_settings['base_url']
            )
            
            # –ü–∞—Ç—á–∏–º —Ñ—É–Ω–∫—Ü–∏–∏ –≤ page_index
            def patched_ChatGPT_API(model=None, prompt=None, api_key=None, chat_history=None):
                max_retries = 10
                # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama
                final_model = ollama_settings['model']
                if model and model != final_model:
                    logger.warning(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å '{model}', –∏—Å–ø–æ–ª—å–∑—É–µ–º '{final_model}' –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama")
                model = final_model
                for i in range(max_retries):
                    try:
                        if chat_history:
                            messages = chat_history.copy()
                            messages.append({"role": "user", "content": prompt})
                        else:
                            messages = [{"role": "user", "content": prompt}]
                        response = ollama_client.chat.completions.create(
                            model=model, messages=messages, temperature=0, timeout=900  # 15 –º–∏–Ω—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        )
                        return response.choices[0].message.content
                    except Exception as e:
                        if i < max_retries - 1:
                            import time
                            time.sleep(1)
                        else:
                            return "Error"
            
            async def patched_ChatGPT_API_async(model=None, prompt=None, api_key=None, chat_history=None):
                max_retries = 10
                # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama
                final_model = ollama_settings['model']
                if model and model != final_model:
                    logger.warning(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å '{model}', –∏—Å–ø–æ–ª—å–∑—É–µ–º '{final_model}' –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama")
                model = final_model
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
                if chat_history:
                    messages = chat_history.copy()
                    messages.append({"role": "user", "content": prompt})
                else:
                    messages = [{"role": "user", "content": prompt}]
                
                for i in range(max_retries):
                    try:
                        response = await ollama_async_client.chat.completions.create(
                            model=model, messages=messages, temperature=0, timeout=900  # 15 –º–∏–Ω—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        )
                        return response.choices[0].message.content
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –≤ async –∑–∞–ø—Ä–æ—Å–µ ({i+1}/{max_retries}): {e}")
                        if i < max_retries - 1:
                            import asyncio
                            await asyncio.sleep(1)
                        else:
                            logger.error(f"Max retries reached for async prompt: {str(prompt)[:100]}")
                            return "Error"
            
            def patched_ChatGPT_API_with_finish_reason(model=None, prompt=None, api_key=None, chat_history=None):
                max_retries = 10
                # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama
                final_model = ollama_settings['model']
                if model and model != final_model:
                    logger.warning(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å '{model}', –∏—Å–ø–æ–ª—å–∑—É–µ–º '{final_model}' –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama")
                model = final_model
                for i in range(max_retries):
                    try:
                        if chat_history:
                            messages = chat_history.copy()
                            messages.append({"role": "user", "content": prompt})
                        else:
                            messages = [{"role": "user", "content": prompt}]
                        response = ollama_client.chat.completions.create(
                            model=model, messages=messages, temperature=0, timeout=900  # 15 –º–∏–Ω—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        )
                        finish_reason = response.choices[0].finish_reason
                        if finish_reason == "length":
                            return response.choices[0].message.content, "max_output_reached"
                        elif finish_reason == "error":
                            # –ï—Å–ª–∏ finish_reason == "error", –ø—Ä–æ–±—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å
                            logger.warning(f"Ollama –≤–µ—Ä–Ω—É–ª finish_reason='error', –ø–æ–≤—Ç–æ—Ä—è—é –∑–∞–ø—Ä–æ—Å ({i+1}/{max_retries})")
                            if i < max_retries - 1:
                                import time
                                time.sleep(1)
                                continue
                            else:
                                logger.error("Max retries reached, finish_reason='error'")
                                return "Error", "error"
                        else:
                            return response.choices[0].message.content, "finished"
                    except Exception as e:
                        if i < max_retries - 1:
                            import time
                            time.sleep(1)
                        else:
                            return "Error", "error"
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á–∏–Ω–≥
            page_index_module.ChatGPT_API = patched_ChatGPT_API
            page_index_module.ChatGPT_API_async = patched_ChatGPT_API_async
            page_index_module.ChatGPT_API_with_finish_reason = patched_ChatGPT_API_with_finish_reason
            logger.info("–ü–∞—Ç—á–∏–Ω–≥ –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ page_index –º–æ–¥—É–ª—é")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ç—á–∏—Ç—å page_index –º–æ–¥—É–ª—å: {e}")
        
except ImportError:
    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–ø—Ä—è–º—É—é
    try:
        from pageindex import page_index_main, config
    except ImportError as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PageIndex: {e}")
        raise

class PageIndexService:
    """Service for PageIndex document indexing and search"""
    
    def __init__(self):
        self.index_dir = Path(settings.INDEX_DIR)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
        if not check_ollama_connection(settings.OLLAMA_BASE_URL):
            logger.warning("‚ö†Ô∏è  Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω! –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω.")
    
    def index_document(
        self,
        pdf_path: str,
        document_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç PDF –¥–æ–∫—É–º–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É—è PageIndex
        
        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        try:
            import os
            if not os.path.exists(pdf_path):
                raise FileNotFoundError(f"PDF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
            
            file_size_mb = os.path.getsize(pdf_path) / 1024 / 1024
            logger.info(f"–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {pdf_path}")
            logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} MB")
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å Ollama: {settings.OLLAMA_MODEL}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
            from pageindex_ollama import check_ollama_connection
            if not check_ollama_connection(settings.OLLAMA_BASE_URL):
                raise ConnectionError("Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω! –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É " + settings.OLLAMA_BASE_URL)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ü–∏–π PageIndex
            opt = config(
                model=settings.OLLAMA_MODEL,
                toc_check_page_num=20,
                max_page_num_each_node=settings.PAGEINDEX_MAX_PAGES_PER_NODE,
                max_token_num_each_node=settings.PAGEINDEX_MAX_TOKENS_PER_NODE,
                if_add_node_id='yes',
                if_add_node_summary='yes',
                if_add_doc_description='no',
                if_add_node_text='no'
            )
            
            logger.info("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ PageIndex –ø—Ä–∏–º–µ–Ω–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...")
            logger.info("–í–ù–ò–ú–ê–ù–ò–ï: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-30 –º–∏–Ω—É—Ç –∏–ª–∏ –±–æ–ª—å—à–µ!")
            logger.info("–ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞–µ—Ç:")
            logger.info("  1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è")
            logger.info("  2. –ü—Ä–æ–≤–µ—Ä–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü")
            logger.info("  3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—é summary –¥–ª—è —É–∑–ª–æ–≤")
            logger.info("  4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            import time
            start_time = time.time()
            
            try:
                result = page_index_main(pdf_path, opt)
            except Exception as indexing_error:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ page_index_main: {indexing_error}")
                import traceback
                logger.error(traceback.format_exc())
                raise
            
            elapsed_time = time.time() - start_time
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥ ({elapsed_time/60:.2f} –º–∏–Ω—É—Ç)")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if not result:
                raise ValueError("PageIndex –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            
            if 'structure' not in result:
                logger.warning("–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'structure', –ø—Ä–æ–≤–µ—Ä—è—é —Ñ–æ—Ä–º–∞—Ç...")
                # –í–æ–∑–º–æ–∂–Ω–æ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
                if isinstance(result, list):
                    result = {'structure': result, 'doc_name': Path(pdf_path).stem}
                else:
                    raise ValueError("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç PageIndex")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
            if document_id:
                index_filename = f"document_{document_id}_index.json"
            else:
                pdf_name = Path(pdf_path).stem
                index_filename = f"{pdf_name}_index.json"
            
            index_path = self.index_dir / index_filename
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            index_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(index_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {index_path}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            structure = result.get('structure', [])
            if structure:
                node_count = self._count_nodes(structure)
                logger.info(f"–°–æ–∑–¥–∞–Ω–æ —É–∑–ª–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ: {node_count}")
            
            return {
                "success": True,
                "index_path": str(index_path),
                "structure": result
            }
            
        except FileNotFoundError:
            raise
        except ConnectionError:
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _count_nodes(self, structure: list) -> int:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ"""
        count = 0
        for node in structure:
            if isinstance(node, dict):
                count += 1
                if 'nodes' in node:
                    count += self._count_nodes(node['nodes'])
        return count
    
    def load_index(self, index_path: str) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            index_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–Ω–¥–µ–∫—Å–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞
        """
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
    
    async def search_tree(
        self,
        index_path: str,
        query: str
    ) -> Dict[str, Any]:
        """
        Reasoning-based –ø–æ–∏—Å–∫ –≤ –¥–µ—Ä–µ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è LLM
        
        Args:
            index_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–Ω–¥–µ–∫—Å–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —É–∑–ª–∞–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not query or not query.strip():
            raise ValueError("–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        if not index_path or not Path(index_path).exists():
            raise FileNotFoundError(f"–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_path}")
        
        try:
            index_data = self.load_index(index_path)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–µ—Ä–µ–≤–∞
            structure = index_data.get('structure', [])
            if not structure:
                logger.warning("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞")
                return {
                    "query": query,
                    "node_list": [],
                    "context": "",
                    "sources": []
                }
            
            # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–µ—Ä–µ–≤–∞ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            tree_without_text = self._remove_fields_from_tree(structure.copy(), fields=['text'])
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è tree search
            import json
            search_prompt = f"""
You are given a question and a tree structure of a document.
Each node contains a node id, node title, and a corresponding summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structure:
{json.dumps(tree_without_text, indent=2, ensure_ascii=False)}

Please reply in the following JSON format:
{{
    "thinking": "<Your thinking process on which nodes are relevant to the question>",
    "node_list": ["node_id_1", "node_id_2", ..., "node_id_n"]
}}
Directly return the final JSON structure. Do not output anything else.
"""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–µ—Ä–µ–≤–∞ - –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ, –æ–±—Ä–µ–∑–∞–µ–º –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            tree_json = json.dumps(tree_without_text, indent=2, ensure_ascii=False)
            max_tree_size = 50000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–µ—Ä–µ–≤–∞ –≤ –ø—Ä–æ–º–ø—Ç–µ
            
            if len(tree_json) > max_tree_size:
                logger.warning(f"–î–µ—Ä–µ–≤–æ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ ({len(tree_json)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞")
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–µ —É—Ä–æ–≤–Ω–∏ –¥–µ—Ä–µ–≤–∞
                tree_without_text = self._truncate_tree_for_search(tree_without_text, max_depth=2)
                tree_json = json.dumps(tree_without_text, indent=2, ensure_ascii=False)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º tree search —á–µ—Ä–µ–∑ Ollama
            from pageindex_ollama import get_ollama_settings, check_ollama_connection
            ollama_settings = get_ollama_settings()
            model = ollama_settings.get('model', settings.OLLAMA_MODEL)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
            if not check_ollama_connection(settings.OLLAMA_BASE_URL):
                logger.warning("Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º keyword search")
                return self._simple_keyword_search(structure, query)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—á–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é ChatGPT_API
            try:
                from PageIndex.pageindex.utils import ChatGPT_API
                tree_search_result = ChatGPT_API(model=model, prompt=search_prompt)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                if not tree_search_result or tree_search_result == "Error":
                    logger.warning("LLM –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º keyword search")
                    return self._simple_keyword_search(structure, query)
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM –¥–ª—è tree search: {e}")
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                return self._simple_keyword_search(structure, query)
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            try:
                from PageIndex.pageindex.utils import extract_json
                tree_search_json = extract_json(tree_search_result)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ tree search: {e}")
                return self._simple_keyword_search(structure, query)
            
            node_list = tree_search_json.get('node_list', [])
            thinking = tree_search_json.get('thinking', '')
            
            logger.info(f"Tree search found {len(node_list)} relevant nodes")
            logger.debug(f"Thinking: {thinking}")
            
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —É–∑–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            node_map = self._create_node_mapping(structure)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —É–∑–ª–æ–≤
            retrieved_nodes = []
            context_parts = []
            sources = []
            
            for node_id in node_list:
                if node_id in node_map:
                    node = node_map[node_id]
                    retrieved_nodes.append(node)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    if 'summary' in node and node['summary']:
                        context_parts.append(f"Section: {node.get('title', 'Unknown')}\n{node['summary']}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                    sources.append({
                        "node_id": node_id,
                        "title": node.get('title', 'Unknown'),
                        "pages": f"{node.get('start_index', 0)}-{node.get('end_index', 0)}"
                    })
            
            context = "\n\n".join(context_parts)
            
            return {
                "query": query,
                "thinking": thinking,
                "node_list": node_list,
                "retrieved_nodes": retrieved_nodes,
                "context": context,
                "sources": sources
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _remove_fields_from_tree(self, tree: Any, fields: list) -> Any:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —É–¥–∞–ª—è–µ—Ç –ø–æ–ª—è –∏–∑ –¥–µ—Ä–µ–≤–∞"""
        if isinstance(tree, dict):
            result = {}
            for key, value in tree.items():
                if key not in fields:
                    result[key] = self._remove_fields_from_tree(value, fields)
            return result
        elif isinstance(tree, list):
            return [self._remove_fields_from_tree(item, fields) for item in tree]
        else:
            return tree
    
    def _create_node_mapping(self, structure: list) -> Dict[str, Dict]:
        """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ node_id -> node –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞"""
        node_map = {}
        
        def traverse(nodes):
            for node in nodes:
                if isinstance(node, dict):
                    node_id = node.get('node_id')
                    if node_id:
                        node_map[node_id] = node
                    if 'nodes' in node:
                        traverse(node['nodes'])
        
        traverse(structure)
        return node_map
    
    def _truncate_tree_for_search(self, tree: Any, max_depth: int = 2, current_depth: int = 0) -> Any:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–µ–∑–∞–µ—Ç –¥–µ—Ä–µ–≤–æ –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if current_depth >= max_depth:
            # –ù–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω–µ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if isinstance(tree, dict):
                return {
                    'node_id': tree.get('node_id'),
                    'title': tree.get('title'),
                    'summary': tree.get('summary', '')[:200] if tree.get('summary') else ''  # –û–±—Ä–µ–∑–∞–µ–º summary
                }
            return tree
        
        if isinstance(tree, dict):
            result = {}
            for key, value in tree.items():
                if key == 'nodes' and current_depth < max_depth:
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã
                    result[key] = [self._truncate_tree_for_search(node, max_depth, current_depth + 1) 
                                  for node in (value if isinstance(value, list) else [])]
                elif key not in ['text']:  # –ò—Å–∫–ª—é—á–∞–µ–º –±–æ–ª—å—à–∏–µ –ø–æ–ª—è
                    result[key] = self._truncate_tree_for_search(value, max_depth, current_depth)
            return result
        elif isinstance(tree, list):
            return [self._truncate_tree_for_search(item, max_depth, current_depth) for item in tree]
        else:
            return tree
    
    def _simple_keyword_search(self, structure: list, query: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∫–∞–∫ fallback"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        relevant_nodes = []
        
        def traverse(nodes):
            for node in nodes:
                if isinstance(node, dict):
                    title = node.get('title', '').lower()
                    summary = node.get('summary', '').lower()
                    
                    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    title_matches = sum(1 for word in query_words if word in title)
                    summary_matches = sum(1 for word in query_words if word in summary)
                    
                    if title_matches > 0 or summary_matches > 0:
                        relevant_nodes.append({
                            'node_id': node.get('node_id'),
                            'title': node.get('title'),
                            'summary': node.get('summary'),
                            'start_index': node.get('start_index'),
                            'end_index': node.get('end_index'),
                            'relevance_score': title_matches * 2 + summary_matches
                        })
                    
                    if 'nodes' in node:
                        traverse(node['nodes'])
        
        traverse(structure)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevant_nodes.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return {
            "query": query,
            "node_list": [n['node_id'] for n in relevant_nodes[:5] if n.get('node_id')],
            "retrieved_nodes": relevant_nodes[:5],
            "context": "\n\n".join([f"Section: {n['title']}\n{n.get('summary', '')}" for n in relevant_nodes[:5]]),
            "sources": [{
                "node_id": n['node_id'],
                "title": n['title'],
                "pages": f"{n.get('start_index', 0)}-{n.get('end_index', 0)}"
            } for n in relevant_nodes[:5]]
        }
